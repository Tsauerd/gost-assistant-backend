# app/ingest.py
from __future__ import annotations

import os
import re
import argparse
from typing import List, Optional, Tuple

import pdfplumber
from dotenv import load_dotenv
from sqlalchemy import text as sql_text

from .db import SessionLocal
from .rag import embed_text  # –í–ê–ñ–ù–û: embed_text –¥–æ–ª–∂–µ–Ω —Å—É—â–µ—Å—Ç–≤–æ–≤–∞—Ç—å –≤ rag.py

load_dotenv()


def clean_cell_text(text: Optional[str]) -> str:
    """–£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø–µ—Ä–µ–Ω–æ—Å—ã –∏ –ø—Ä–æ–±–µ–ª—ã –¥–ª—è '–ø–ª–æ—Å–∫–æ–≥–æ' —Ç–µ–∫—Å—Ç–∞."""
    if not text:
        return ""
    return " ".join(str(text).strip().split())


def page_to_markdown(page) -> str:
    """
    –¢–µ–∫—Å—Ç + —Ç–∞–±–ª–∏—Ü—ã —Å–æ —Å—Ç—Ä–∞–Ω–∏—Ü—ã.
    –¢–∞–±–ª–∏—Ü—ã: Markdown + –≤–µ—Ä–±–∞–ª–∏–∑–∞—Ü–∏—è (–¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞).
    """
    parts: List[str] = []

    # 1) –¢–µ–∫—Å—Ç
    text = page.extract_text() or ""
    text = text.strip()
    if text:
        parts.append(text)

    # 2) –¢–∞–±–ª–∏—Ü—ã
    tables = page.extract_tables()
    for t_idx, table in enumerate(tables or []):
        if not table or not table[0]:
            continue

        header_raw = table[0]
        rows = table[1:] if len(table) > 1 else []

        clean_headers = [clean_cell_text(h) for h in header_raw]

        md_lines: List[str] = []
        md_header = "| " + " | ".join(clean_cell_text(cell) for cell in header_raw) + " |"
        md_sep = "| " + " | ".join("---" for _ in header_raw) + " |"
        md_lines.append(md_header)
        md_lines.append(md_sep)

        verbalized_rows: List[str] = []

        for row in rows:
            row = row or []
            clean_row = [clean_cell_text(cell) for cell in row]
            # markdown —Å—Ç—Ä–æ–∫–∞ (–ø–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤—É –∫–æ–ª–æ–Ω–æ–∫ –∫–∞–∫ header)
            padded = (clean_row + [""] * len(clean_headers))[:len(clean_headers)]
            md_lines.append("| " + " | ".join(padded) + " |")

            row_pairs = []
            for h, cell in zip(clean_headers, padded):
                if h and cell:
                    row_pairs.append(f"{h}: {cell}")
            if row_pairs:
                verbalized_rows.append("; ".join(row_pairs) + ".")

        md_table_block = "\n".join(md_lines)
        verbalized_text_block = "\n".join(verbalized_rows).strip() or "(–Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö)"

        table_output = (
            f"\n–¢–∞–±–ª–∏—Ü–∞ {t_idx + 1}:\n"
            f"{md_table_block}\n\n"
            f"–û–ø–∏—Å–∞–Ω–∏–µ —Å—Ç—Ä–æ–∫ —Ç–∞–±–ª–∏—Ü—ã {t_idx + 1}:\n"
            f"{verbalized_text_block}\n"
        )
        parts.append(table_output)

    return "\n\n".join(parts).strip()


def read_pdf_with_tables(pdf_path: str) -> str:
    """–°—á–∏—Ç—ã–≤–∞–µ–º PDF: —Ç–µ–∫—Å—Ç + —Ç–∞–±–ª–∏—Ü—ã."""
    all_parts: List[str] = []
    with pdfplumber.open(pdf_path) as pdf:
        for page_idx, page in enumerate(pdf.pages):
            page_md = page_to_markdown(page)
            if page_md:
                all_parts.append(f"=== –°—Ç—Ä–∞–Ω–∏—Ü–∞ {page_idx + 1} ===\n{page_md}")
    return "\n\n".join(all_parts)


def split_text_to_chunks(text: str, max_chars: int = 2000) -> List[str]:
    """
    –ß–∞–Ω–∫–∏–Ω–≥ –ø–æ –ø—É—Å—Ç—ã–º —Å—Ç—Ä–æ–∫–∞–º (–∞–±–∑–∞—Ü–∞–º).
    –î–ª–∏–Ω–Ω—ã–µ –±–ª–æ–∫–∏ —Ä–µ–∂–µ–º –ø–æ —Å–∏–º–≤–æ–ª–∞–º.
    """
    blocks = [b.strip() for b in re.split(r"\n\s*\n", text) if b.strip()]

    chunks: List[str] = []
    current = ""

    for block in blocks:
        candidate = (current + "\n\n" + block).strip() if current else block

        if len(candidate) <= max_chars:
            current = candidate
        else:
            if current:
                chunks.append(current)

            if len(block) <= max_chars:
                current = block
            else:
                start = 0
                while start < len(block):
                    end = start + max_chars
                    chunks.append(block[start:end].strip())
                    start = end
                current = ""

    if current:
        chunks.append(current)

    return chunks


def extract_section_paragraph(chunk_text: str) -> Tuple[Optional[str], Optional[str]]:
    """
    section: "5"
    paragraph: "5.2.1"
    """
    section = None
    paragraph = None

    lines = [l.strip() for l in chunk_text.splitlines() if l.strip()]
    if not lines:
        return None, None

    full_text = "\n".join(lines)

    para_match = re.search(r"^(\d+(?:\.\d+){1,3})\s", full_text, re.MULTILINE)
    if para_match:
        paragraph = para_match.group(1)

    sec_match = re.search(r"^(\d{1,2})\s+[–ê-–Ø–ÅA-Z]", lines[0])
    if sec_match:
        section = sec_match.group(1)

    return section, paragraph


def purge_document(db, standard_number: str, year: Optional[int]) -> int:
    """
    –£–¥–∞–ª—è–µ—Ç –∏–∑ –ë–î –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —á–∞–Ω–∫–∏ –ø–æ standard_number (+ year –µ—Å–ª–∏ –∑–∞–¥–∞–Ω).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å–ª–æ —É–¥–∞–ª—ë–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤.
    """
    where = "standard_number = :std"
    params = {"std": standard_number}
    if year is not None:
        where += " AND year = :year"
        params["year"] = year

    ids = db.execute(
        sql_text(f"SELECT id FROM documents WHERE {where}"),
        params,
    ).scalars().all()

    if not ids:
        return 0

    db.execute(
        sql_text("DELETE FROM document_chunks WHERE document_id = ANY(:ids)"),
        {"ids": ids},
    )
    db.execute(
        sql_text("DELETE FROM documents WHERE id = ANY(:ids)"),
        {"ids": ids},
    )
    return len(ids)


def ingest_pdf(
    pdf_path: str,
    standard_number: str,
    year: int,
    doc_name: Optional[str] = None,
    purge: bool = False,
    source: str = "manual_ingest",
):
    if doc_name is None:
        doc_name = os.path.basename(pdf_path)

    print(f"–ß–∏—Ç–∞–µ–º PDF (—Ç–µ–∫—Å—Ç + —Ç–∞–±–ª–∏—Ü—ã): {pdf_path}")
    full_text = read_pdf_with_tables(pdf_path)
    print(f"–î–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞: {len(full_text)} —Å–∏–º–≤–æ–ª–æ–≤")

    chunks = split_text_to_chunks(full_text, max_chars=2000)
    print(f"–ß–∞–Ω–∫–æ–≤: {len(chunks)}")

    insert_doc_sql = sql_text("""
        INSERT INTO documents (name, standard_number, year, source)
        VALUES (:name, :std, :year, :src)
        RETURNING id;
    """)

    insert_chunk_sql = sql_text("""
        INSERT INTO document_chunks (document_id, chunk_index, text, section, paragraph, embedding)
        VALUES (:doc_id, :idx, :text, :section, :paragraph, :embedding)
    """)

    # –í–ê–ñ–ù–û: –æ–¥–Ω–∞ —Å–µ—Å—Å–∏—è –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç = –±—ã—Å—Ç—Ä–µ–µ
    with SessionLocal() as db:
        if purge:
            deleted = purge_document(db, standard_number=standard_number, year=year)
            db.commit()
            if deleted:
                print(f"üßπ purge: —É–¥–∞–ª–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {deleted}")

        doc_id = db.execute(insert_doc_sql, {
            "name": doc_name,
            "std": standard_number,
            "year": year,
            "src": source,
        }).scalar()
        db.commit()
        print(f"–°–æ–∑–¥–∞–Ω documents.id = {doc_id}")

        for i, raw_chunk in enumerate(chunks):
            if i % 10 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–∞ {i+1}/{len(chunks)}...")

            section, paragraph = extract_section_paragraph(raw_chunk)

            header_parts = [f"–ì–û–°–¢ {standard_number}", str(year)]
            if section:
                header_parts.append(f"–†–∞–∑–¥–µ–ª {section}")
            if paragraph:
                header_parts.append(f"–ü—É–Ω–∫—Ç {paragraph}")

            header = " | ".join(header_parts)
            chunk_text = f"[{header}]\n{raw_chunk}"

            emb = embed_text(chunk_text)  # –º–æ–¥–µ–ª—å –¥–æ–ª–∂–Ω–∞ —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å rag.py

            # pgvector literal
            emb_literal = "[" + ",".join(f"{x:.6f}" for x in emb) + "]"

            db.execute(insert_chunk_sql, {
                "doc_id": doc_id,
                "idx": i,
                "text": chunk_text,
                "section": section,
                "paragraph": paragraph,
                "embedding": emb_literal,
            })

            # –∫–æ–º–º–∏—Ç–∏–º –ø–æ—Ä—Ü–∏—è–º–∏, —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞—Ç—å –≥–∏–≥–∞–Ω—Ç—Å–∫—É—é —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏—é
            if (i + 1) % 50 == 0:
                db.commit()

        db.commit()

    print("‚úÖ –ì–æ—Ç–æ–≤–æ: –¥–æ–∫—É–º–µ–Ω—Ç –∏ –≤—Å–µ —á–∞–Ω–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω—ã.")


def main():
    parser = argparse.ArgumentParser(description="Ingest GOST PDF into DB")
    parser.add_argument("--file", required=True, help="–ü—É—Ç—å –∫ PDF")
    parser.add_argument("--std", required=True, help="–ù–æ–º–µ—Ä —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, '–ì–û–°–¢ 12345-2020')")
    parser.add_argument("--year", required=True, type=int, help="–ì–æ–¥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞")
    parser.add_argument("--name", help="–ù–∞–∑–≤–∞–Ω–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é ‚Äî –∏–º—è —Ñ–∞–π–ª–∞)")
    parser.add_argument("--purge", action="store_true", help="–£–¥–∞–ª–∏—Ç—å —Å—Ç–∞—Ä—É—é –≤–µ—Ä—Å–∏—é —ç—Ç–æ–≥–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π")
    args = parser.parse_args()

    ingest_pdf(
        pdf_path=args.file,
        standard_number=args.std,
        year=args.year,
        doc_name=args.name,
        purge=args.purge,
    )


if __name__ == "__main__":
    main()
